# micrograd-Tiny-Autograd-Engine-Neural-Net-from-Scratch
Inspired by Andrej Karpathyâ€™s micrograd, this project implements a minimalistic autograd engine and a small feedforward neural network â€” all in under 100 lines of Python.

##  visualization

![2d neuron](gout.svg)


## ğŸ” About Micrograd

[**micrograd**](https://github.com/karpathy/micrograd) is a minimalist, educational autograd engine and neural network library (MITâ€‘licensed), authored by **Andrej Karpathy**. It implements reverseâ€‘mode automatic differentiation (backpropagation) over a dynamically constructed computation graph of **scalar `Value` objects**, and includes a tiny multilayer perceptron with a PyTorchâ€‘like APIâ€”all in under ~200 lines of Pythonâ€‚:contentReference[oaicite:1]{index=1}.

### Key Features
- **`Value` class** that tracks data and gradients, overloads operations (+, âˆ’, \*, **, tanh, exp, etc.)
- **Automatic topological sorting** for gradient propagation (`backward()`), implementing the chain ruleâ€‚:contentReference[oaicite:2]{index=2}
- **Small network library**: defines `Neuron`, `Layer`, and `MLP` classes to build and train feedâ€‘forward networksâ€‚:contentReference[oaicite:3]{index=3}
- Excellent for learning how deep learning frameworks compute gradients from first principles

---

## ğŸ¥ Lecture: Building Micrograd

Karpathy walks through the implementation stepâ€‘byâ€‘step in his â€œNeural Networks: Zero to Heroâ€ YouTube lecture:

[The spelledâ€‘out intro to neural networks and backpropagation: building micrograd](https://www.semanlink.net/doc/2022/11/the_spelled_out_intro_to_neural?utm_source=chatgpt.com)


In this ~2â€¯h 25â€¯m video he starts from scratch with a blank Jupyter notebook and builds up micrograd, explaining Value objects, manual backprop, and training a small MLP model on toy dataâ€‚:contentReference[oaicite:5]{index=5}.

---

## ğŸ”— Citations & Links

- **GitHub**: [karpathy/micrograd](https://github.com/karpathy/micrograd)â€‚:contentReference[oaicite:6]{index=6}  
- **Lecture**: [YouTube â€“ Building micrograd from scratch]:contentReference[oaicite:7]{index=7}

---

## ğŸ“ Attribution

This project is inspired by, or based on, Andrej Karpathyâ€™s micrograd. Full credit to the original author under the MIT license.

